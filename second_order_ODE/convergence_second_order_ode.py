import numpy as np
import cvxpy as cp
import matplotlib.pyplot as plt


def compute_convergence_guarantee(mu, L, alpha, beta, a=1., P=None, psd=False, upper=2., lower=0., epsilon=10 ** -4):
    """
    Consider the second-order gradient flow
            d^2x/dt2 + beta * dx/dt + alpha * grad f(x(t)) = 0,
    where alpha, beta are positive parameters, and f are L-smooth mu-strongly convex functions.

    We consider quadratic Lyapunov functions
        V(x(t)) = a(f(x(t)) - f^*) + X^TPX,
        X = (x(t)-x^*, dx/dt),
    where a is a positive parameter, and P is a symmetric matrix, such that V(x(t)) is positive over the trajectory.

    This code computes the worst-case guarantee tau(L, mu, a, alpha, beta, P) such that,
        d/dt V(x(t)) <= -tau(L, mu, a, alpha, beta, P) V(x(t)),
    for all L-smooth and mu-strongly convex functions, and all ODEs generated by the second-order gradient flow.

    When P=None, this code optimizes over the family of quadratic Lyapunov function.

    :param mu: strong convexity assumption
    :param L: smoothness assumption
    :param alpha: ode parameter
    :param beta: ode parameter
    :param a: lyapunov parameter (one has to be fixed)
    :param P: lyapunov parameter matrix - is optimized when equal to None
    :param psd: P must be psd when set to True, relaxed otherwise.
    :param upper: upper bound on the convergence rate
    :param lower: lower bound on the convergence rate
    :param epsilon: precision
    :return:
            - worst-case guarantee tau(L, mu, alpha, beta, P)
            - P
            - dual variables associated with interpolation inequalities (decreasing derivatives)
            - dual variables associated with positivy constraints

    """
    ## PARAMETERS
    npt = 2  # optimal and initial point
    dimF = 1  # one dimension only
    dimG = 3  # (x(t) // x_dot(t) // grad f(x(t)))
    dimP = 2

    ## INITIALIZE
    FF = []
    GG = []
    YY = []
    # optimal point : equal to zero
    YY.append(np.zeros((1, dimG)))
    FF.append(np.zeros((1, dimF)))
    GG.append(np.zeros((1, dimG)))
    # initial point
    y = np.zeros((1, dimG))
    y[0][0] = 1.
    YY.append(y)
    # construction of the base
    f = np.zeros((1, dimF))
    f[0][0] = 1.
    FF.append(f)
    g = np.zeros((1, dimG))
    g[0][2] = 1.
    GG.append(g)

    ## GET THE INEQUALITIES
    A = []
    b = []
    for i in range(npt):
        for j in range(npt):
            if j != i:
                if (mu != 0 & L != 0):
                    Aij = np.dot((YY[i] - YY[j]).T, GG[j]) + \
                          1 / 2 / (1 - mu / L) * (1 / L * np.dot((GG[i] - GG[j]).T, GG[i] - GG[j]) +
                                                  mu * np.dot((YY[i] - YY[j]).T, YY[i] - YY[j]) -
                                                  2 * mu / L * np.dot((YY[i] - YY[j]).T, GG[i] - GG[j]))
                if (mu != 0 & L == 0):
                    Aij = np.dot((YY[i] - YY[j]).T, GG[j]) + \
                          1 / 2 * mu * np.dot((YY[i] - YY[j]).T, YY[i] - YY[j])
                if (mu == 0 & L != 0):
                    Aij = np.dot((YY[i] - YY[j]).T, GG[j]) + \
                          1 / 2 / L * np.dot((GG[i] - GG[j]).T, GG[i] - GG[j])
                if (mu == 0 & L == 0):
                    Aij = np.dot((YY[i] - YY[j]).T, GG[j])
                A.append(.5 * (Aij + Aij.T))
                b.append(FF[j] - FF[i])

    ## Apply the ODE and create a dimension for Y_dot
    Y_dot = np.zeros((1, dimG))
    Y_dot[0][1] = 1.
    Y_dot_dot = - GG[1] * alpha - beta * Y_dot
    X_0 = np.array([YY[1], Y_dot])
    X_1 = np.array([Y_dot, Y_dot_dot])

    if P is None:
        P_ = None
        l_ = None
        nu_ = None
        while upper - lower >= epsilon:
            tau = (lower + upper) / 2
            # Variables in CVXPY
            P = cp.Variable((dimP, dimP), symmetric=True)
            l = cp.Variable(npt * (npt - 1))  # dual values : interpolation inequalities
            if not psd:
                nu = cp.Variable(npt * (npt - 1)) # dual values : positivity constraints
            # CONSTRAINTS
            ### Positivity of the interpolation inequalities
            constraints = [l <= 0.]
            #if not psd:
            #    constraints = constraints + [nu >= 0]
            ### Constraints for positivity of the Lyapunov
            if not psd:
                constraints = constraints + [X_0[:, 0].T @ P @ X_0[:, 0] + sum([nu[i] * A[i] for i in range(len(A))]) >> 0.]  # P is psd
                constraints = constraints + [sum([nu[i] * b[i][0] for i in range(len(b))]) + a * FF[1][0] >= 0.]
            else:
                constraints = constraints + [P >> 0]
            ### Constraints on the lyapunov
            constraints = constraints + [sum([l[i] * A[i] for i in range(len(A))])
                                         + X_1[:, 0].T @ P @ X_0[:, 0] + X_0[:, 0].T @ P @ X_1[:, 0]
                                         + a * (Y_dot.T @ GG[1] + GG[1].T @ Y_dot) / 2
                                         + tau * X_0[:, 0].T @ P @ X_0[:, 0] << 0.]  # derivative of f(x(t))
            constraints = constraints + [sum([l[i] * b[i][0] for i in range(len(b))])
                                         + tau * (a * FF[1][0]) == 0.]
            # OPTIMIZE
            prob = cp.Problem(cp.Minimize(0.), constraints)
            prob.solve(cp.SCS)
            if prob.status == 'optimal':
                lower = tau
                P_ = P.value
                l_ = l.value
                if not psd:
                    nu_ = nu.value
            else:
                upper = tau
    else:
        P_ = P
        l_ = None
        nu_ = None
        while upper - lower >= epsilon:
            tau = (lower + upper) / 2
            # Variables in CVXPY
            l = cp.Variable(npt * (npt - 1))  # dual values : interpolation inequalities
            # CONSTRAINTS
            ### Positivity of the interpolation inequalities
            constraints = [l <= 0]
            ### Constraints on the lyapunov
            constraints = constraints + [sum([l[i] * A[i] for i in range(len(A))])
                                         + X_1[:, 0].T @ P @ X_0[:, 0] + X_0[:, 0].T @ P @ X_1[:, 0]
                                         + a * (Y_dot.T @ GG[1] + GG[1].T @ Y_dot) / 2
                                         + tau * X_0[:, 0].T @ P @ X_0[:, 0] << 0]  # derivative of V(x(t))
            constraints = constraints + [sum([l[i] * b[i][0] for i in range(len(b))])
                                         + tau * a * (FF[1][0] - FF[0][0]) == 0]
            # OPTIMIZE
            prob = cp.Problem(cp.Minimize(0.), constraints)
            prob.solve(cp.SCS)
            if prob.status == 'optimal':
                lower = tau
                l_ = l.value
            else:
                upper = tau

    return tau, P_, l_, nu_

def compute_convergence_primal(mu, L, alpha, beta, P, a=1):
    """
    Consider the second-order gradient flow
            d^2x/dt2 + beta * dx/dt + alpha * grad f(x(t)) = 0,
    where alpha, beta are positive parameters, and f are L-smooth mu-strongly convex functions.

    We consider quadratic Lyapunov functions
        V(x(t)) = a(f(x(t)) - f^*) + X^TPX,
        X = (x(t)-x^*, dx/dt),
    where a is a positive parameter, and P is a symmetric matrix, such that V(x(t)) is positive over the trajectory.

    This code computes the worst-case guarantee tau(L, mu, a, alpha, beta, P) such that,
        d/dt V(x(t)) <= -tau(L, mu, a, alpha, beta, P) V(x(t)),
    for all L-smooth and mu-strongly convex functions, and all ODEs generated by the second-order gradient flow.


    :param mu: strong convexity parameter
    :param L: smoothness parameter
    :param alpha: ode parameter
    :param beta: ode parameter
    :param a: Lyapunov parameter
    :param P: Lyapunov parameter
    :return:
    """
    ## PARAMETERS
    npt = 2  # optimal and initial point
    dimF = 1  # one dimension only
    dimG = 3  # (x(t) // x_dot(t) // grad f(x(t)))

    ## INITIALIZE
    FF = []
    GG = []
    YY = []
    # optimal point : equal to zero
    YY.append(np.zeros((1, dimG)))
    FF.append(np.zeros((1, dimF)))
    GG.append(np.zeros((1, dimG)))
    # initial point
    y = np.zeros((1, dimG))
    y[0][0] = 1.
    YY.append(y)
    # construction of the base
    f = np.zeros((1, dimF))
    f[0][0] = 1.
    FF.append(f)
    g = np.zeros((1, dimG))
    g[0][2] = 1.
    GG.append(g)

    # Apply the ODE and create a dimension for Y_dot
    Y_dot = np.zeros((1, dimG))
    Y_dot[0][1] = 1.
    Y_dot_dot = -GG[1] * alpha - beta * Y_dot

    X_0 = np.array([YY[1], Y_dot])
    X_1 = np.array([Y_dot, Y_dot_dot])

    # VARIABLES
    G = cp.Variable((dimG, dimG), symmetric=True)
    F = cp.Variable((dimF, 1))

    lyap = a * (FF[1] @ F[:, 0])[0] + sum([(X_0[:, 0].T @ P @ X_0[:, 0] @ G)[k, k] for k in range(dimG)])
    S = a * (Y_dot.T @ GG[1] + GG[1].T @ Y_dot) / 2 + X_1[:, 0].T @ P @ X_0[:, 0] + X_0[:, 0].T @ P @ X_1[:, 0]
    lyap_dot = sum([(S @ G)[k, k] for k in range(dimG)])

    # CONSTRAINTS
    ### Positivity of the Gram matrix
    constraints = [G >> 0.]
    ### Constraint on the previous iterate :
    constraints = constraints + [lyap == 1.]
    ### Interpolation inequalities
    for i in range(npt):
        for j in range(npt):
            if j != i:
                if (mu != 0 & L != 0):
                    Aij = np.dot((YY[i] - YY[j]).T, GG[j]) + \
                          1 / 2 / (1 - mu / L) * (1 / L * np.dot((GG[i] - GG[j]).T, GG[i] - GG[j]) +
                                                  mu * np.dot((YY[i] - YY[j]).T, YY[i] - YY[j]) -
                                                  2 * mu / L * np.dot((YY[i] - YY[j]).T, GG[i] - GG[j]))
                if (mu != 0 & L == 0):
                    Aij = np.dot((YY[i] - YY[j]).T, GG[j]) + \
                          1 / 2 * mu * np.dot((YY[i] - YY[j]).T, YY[i] - YY[j])
                if (mu == 0 & L != 0):
                    Aij = np.dot((YY[i] - YY[j]).T, GG[j]) + \
                          1 / 2 / L * np.dot((GG[i] - GG[j]).T, GG[i] - GG[j])
                if (mu == 0 & L == 0):
                    Aij = np.dot((YY[i] - YY[j]).T, GG[j])

                A = .5 * (Aij + Aij.T)
                b = FF[j] - FF[i]
                constraints += [b[0] @ F[:, 0] + sum([(A @ G)[k, k] for k in range(dimG)]) <= 0.]

    ## OPTIMIZE
    prob = cp.Problem(cp.Maximize(lyap_dot), constraints)
    prob.solve(cp.SCS)
    return prob.value, G.value, F.value


if __name__ == '__main__':
    mu = 0.001
    L = 0
    alpha = 1
    beta = 2 * np.sqrt(mu)
    a = 1.

    ## RELAXED CONDITION ON P
    print('RELAXED')
    # Compute worst-case guarantee when the condition P >> 0 is relaxed
    relaxed_tau, relaxed_P, _, nu = compute_convergence_guarantee(mu=mu,
                                                  L=L,
                                                  alpha=alpha,
                                                  beta=beta,
                                                  a=a,
                                                  psd=False)
    print('Convergence guarantee while optimizing over P', relaxed_tau, 4/3 * np.sqrt(mu))

    # Verify the estimated value for P (relaxed)
    P = np.zeros((2, 2))
    P[0][0] = 4 / 9 * mu
    P[1][0] = 2 / 3 * np.sqrt(mu)
    P[0][1] = 2 / 3 * np.sqrt(mu)
    P[1][1] = 1 / 2
    relaxed_tau_verif, _, _, _ = compute_convergence_guarantee(mu=mu,
                                                              L=L,
                                                              alpha=alpha,
                                                              beta=beta,
                                                              a=a,
                                                              P=P)
    print('Convergence guarantee for fixed P (relaxed)', relaxed_tau_verif, 4 / 3 * np.sqrt(mu))

    # Verify in the primal
    primal_relaxed_tau, _, _ = compute_convergence_primal(mu=mu,
                                                          L=L,
                                                          alpha=alpha,
                                                          beta=beta,
                                                          P=P)
    print('Primal : convergence guarantee for fixed P (relaxed)', -primal_relaxed_tau, 4 / 3 * np.sqrt(mu))

    ## POSITIVITY CONDITION ON P
    print(' ')
    print('P PSD')
    # Compute worst-case guarantee when P >> 0
    tau, _, _, _ = compute_convergence_guarantee(mu=mu,
                                                  L=L,
                                                  alpha=alpha,
                                                  beta=beta,
                                                  a=a,
                                                  psd=True)
    print('Convergence guarantee while optimizing over P (psd)', tau, np.sqrt(mu))

    # Verify the estimated value for P
    P = np.zeros((2, 2))
    P[0][0] = mu / 2
    P[1][0] = np.sqrt(mu) / 2
    P[0][1] = np.sqrt(mu) / 2
    P[1][1] = 1 / 2
    tau_verif, _, _, _ = compute_convergence_guarantee(mu=mu,
                                                      L=L,
                                                      alpha=alpha,
                                                      beta=beta,
                                                      a=a,
                                                      P=P)
    print('Convergence guarantee for fixed P (psd)', tau_verif, np.sqrt(mu))

    # Verify in the primal
    primal_tau, _, _ = compute_convergence_primal(mu=mu,
                                                  L=L,
                                                  alpha=alpha,
                                                  beta=beta,
                                                  P=P)
    print('Primal : convergence guarantee for fixed P (psd) ', -primal_tau, np.sqrt(mu))
    print(-primal_tau/np.sqrt(mu))


